"""
Fluxo de manuten√ß√£o e limpeza do Data Lake
"""

import os
from datetime import datetime, timedelta
from typing import Dict, List

from prefect import flow, get_run_logger, task
from prefect.task_runners import ConcurrentTaskRunner


@task(name="üßπ Cleanup Old Partitions", retries=2)
def cleanup_old_partitions(layer: str, retention_days: int = 30) -> Dict:
    """
    Remove parti√ß√µes antigas baseadas na pol√≠tica de reten√ß√£o
    """
    run_logger = get_run_logger()
    run_logger.info(
        f"Limpando parti√ß√µes antigas da camada {layer} (>{retention_days} dias)"
    )

    try:
        # Simular limpeza de parti√ß√µes
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        cleanup_result = {
            "layer": layer,
            "retention_days": retention_days,
            "cutoff_date": cutoff_date.isoformat(),
            "partitions_found": 45,
            "partitions_deleted": 12,
            "space_freed_gb": 2.3,
            "cleanup_duration_seconds": 34.5,
            "cleaned_at": datetime.now().isoformat(),
        }

        run_logger.info(
            f"‚úÖ Limpeza {layer}: {cleanup_result['partitions_deleted']} parti√ß√µes removidas"
        )
        return cleanup_result

    except Exception as e:
        run_logger.error(f"Erro na limpeza {layer}: {str(e)}")
        raise


@task(name="üìä Optimize Delta Tables")
def optimize_delta_tables(layer: str) -> Dict:
    """
    Otimiza tabelas Delta para melhor performance
    """
    run_logger = get_run_logger()
    run_logger.info(f"Otimizando tabelas Delta da camada {layer}")

    try:
        # Simular otimiza√ß√£o Delta
        optimization_result = {
            "layer": layer,
            "tables_found": 8,
            "tables_optimized": 6,
            "files_compacted": 234,
            "size_before_gb": 5.2,
            "size_after_gb": 3.1,
            "compression_ratio": 0.6,
            "optimization_duration_seconds": 125.3,
            "optimized_at": datetime.now().isoformat(),
        }

        space_saved = (
            optimization_result["size_before_gb"] - optimization_result["size_after_gb"]
        )
        run_logger.info(f"‚úÖ Otimiza√ß√£o {layer}: {space_saved:.1f}GB economizados")
        return optimization_result

    except Exception as e:
        run_logger.error(f"Erro na otimiza√ß√£o {layer}: {str(e)}")
        raise


@task(name="üîç Data Quality Audit")
def data_quality_audit(layer: str) -> Dict:
    """
    Executa auditoria de qualidade dos dados
    """
    run_logger = get_run_logger()
    run_logger.info(f"Executando auditoria de qualidade da camada {layer}")

    try:
        # Simular auditoria de qualidade
        quality_audit = {
            "layer": layer,
            "tables_audited": 8,
            "total_records": 1250000,
            "null_percentage": 2.1,
            "duplicate_percentage": 0.8,
            "schema_violations": 3,
            "data_quality_score": 0.94,
            "issues_found": [
                {
                    "table": "stocks_raw_20250101",
                    "issue": "Missing values in volume column",
                    "severity": "low",
                },
                {
                    "table": "stocks_cleaned_20250102",
                    "issue": "Duplicate records",
                    "severity": "medium",
                },
                {
                    "table": "stocks_analytics_20250103",
                    "issue": "Schema mismatch",
                    "severity": "high",
                },
            ],
            "audit_duration_seconds": 78.2,
            "audited_at": datetime.now().isoformat(),
        }

        run_logger.info(
            f"‚úÖ Auditoria {layer}: Score {quality_audit['data_quality_score']:.2%}"
        )
        if quality_audit["issues_found"]:
            run_logger.warning(
                f"‚ö†Ô∏è {len(quality_audit['issues_found'])} problemas encontrados"
            )

        return quality_audit

    except Exception as e:
        run_logger.error(f"Erro na auditoria {layer}: {str(e)}")
        raise


@task(name="üìà Generate Usage Statistics")
def generate_usage_statistics() -> Dict:
    """
    Gera estat√≠sticas de uso do Data Lake
    """
    run_logger = get_run_logger()
    run_logger.info("Gerando estat√≠sticas de uso do Data Lake")

    try:
        # Simular coleta de estat√≠sticas
        usage_stats = {
            "total_size_gb": 12.8,
            "total_tables": 24,
            "total_records": 3750000,
            "daily_ingestion_gb": 0.8,
            "query_frequency": {"bronze": 45, "silver": 123, "gold": 89},
            "top_accessed_tables": [
                {"table": "stocks_analytics_latest", "access_count": 156},
                {"table": "stocks_cleaned_latest", "access_count": 89},
                {"table": "stocks_raw_latest", "access_count": 67},
            ],
            "storage_by_layer": {
                "bronze": {"size_gb": 6.2, "percentage": 48.4},
                "silver": {"size_gb": 4.1, "percentage": 32.0},
                "gold": {"size_gb": 2.5, "percentage": 19.6},
            },
            "generated_at": datetime.now().isoformat(),
        }

        run_logger.info(f"‚úÖ Estat√≠sticas: {usage_stats['total_size_gb']:.1f}GB total")
        return usage_stats

    except Exception as e:
        run_logger.error(f"Erro nas estat√≠sticas: {str(e)}")
        raise


@task(name="üîí Backup Critical Data")
def backup_critical_data() -> Dict:
    """
    Faz backup dos dados cr√≠ticos
    """
    run_logger = get_run_logger()
    run_logger.info("Executando backup de dados cr√≠ticos")

    try:
        # Simular backup
        backup_result = {
            "backup_id": f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "backup_location": "s3://datalab-backup/",
            "tables_backed_up": 12,
            "total_size_gb": 4.2,
            "backup_type": "incremental",
            "backup_duration_seconds": 156.7,
            "backup_status": "success",
            "retention_policy": "30 days",
            "backed_up_at": datetime.now().isoformat(),
        }

        run_logger.info(f"‚úÖ Backup conclu√≠do: {backup_result['backup_id']}")
        return backup_result

    except Exception as e:
        run_logger.error(f"Erro no backup: {str(e)}")
        raise


@task(name="üìä Create Maintenance Report")
def create_maintenance_report(
    cleanup_results: List[Dict],
    optimization_results: List[Dict],
    audit_results: List[Dict],
    usage_stats: Dict,
    backup_result: Dict,
) -> Dict:
    """
    Cria relat√≥rio consolidado de manuten√ß√£o
    """
    run_logger = get_run_logger()
    run_logger.info("Criando relat√≥rio de manuten√ß√£o")

    try:
        # Calcular totais
        total_space_freed = sum(r["space_freed_gb"] for r in cleanup_results)
        total_space_optimized = sum(
            r["size_before_gb"] - r["size_after_gb"] for r in optimization_results
        )
        avg_quality_score = sum(r["data_quality_score"] for r in audit_results) / len(
            audit_results
        )

        # Contar problemas cr√≠ticos
        critical_issues = []
        for audit in audit_results:
            critical_issues.extend(
                [
                    issue
                    for issue in audit["issues_found"]
                    if issue["severity"] == "high"
                ]
            )

        maintenance_report = {
            "report_date": datetime.now().isoformat(),
            "summary": {
                "total_space_freed_gb": total_space_freed,
                "total_space_optimized_gb": total_space_optimized,
                "average_quality_score": avg_quality_score,
                "critical_issues_count": len(critical_issues),
                "backup_status": backup_result["backup_status"],
            },
            "cleanup_summary": {
                "layers_cleaned": len(cleanup_results),
                "total_partitions_deleted": sum(
                    r["partitions_deleted"] for r in cleanup_results
                ),
                "total_space_freed_gb": total_space_freed,
            },
            "optimization_summary": {
                "layers_optimized": len(optimization_results),
                "total_tables_optimized": sum(
                    r["tables_optimized"] for r in optimization_results
                ),
                "total_files_compacted": sum(
                    r["files_compacted"] for r in optimization_results
                ),
                "space_saved_gb": total_space_optimized,
            },
            "quality_summary": {
                "average_score": avg_quality_score,
                "critical_issues": critical_issues,
                "total_issues": sum(len(r["issues_found"]) for r in audit_results),
            },
            "usage_summary": usage_stats,
            "backup_summary": backup_result,
            "recommendations": [],
        }

        # Gerar recomenda√ß√µes
        if avg_quality_score < 0.9:
            maintenance_report["recommendations"].append(
                "Investigar problemas de qualidade - score abaixo de 90%"
            )

        if len(critical_issues) > 0:
            maintenance_report["recommendations"].append(
                f"Resolver {len(critical_issues)} problemas cr√≠ticos imediatamente"
            )

        if usage_stats["total_size_gb"] > 50:
            maintenance_report["recommendations"].append(
                "Considerar arquivamento de dados antigos - uso acima de 50GB"
            )

        run_logger.info(
            f"‚úÖ Relat√≥rio criado - Score qualidade: {avg_quality_score:.2%}"
        )
        return maintenance_report

    except Exception as e:
        run_logger.error(f"Erro no relat√≥rio: {str(e)}")
        raise


@flow(
    name="üßπ Data Lake Maintenance Pipeline",
    description="Pipeline de manuten√ß√£o e limpeza do Data Lake",
    task_runner=ConcurrentTaskRunner(),
)
def data_lake_maintenance_flow(retention_days: int = 30):
    """
    Fluxo de manuten√ß√£o completa do Data Lake

    Args:
        retention_days: Dias de reten√ß√£o para limpeza de parti√ß√µes
    """
    run_logger = get_run_logger()
    run_logger.info("üßπ Iniciando manuten√ß√£o do Data Lake")

    try:
        layers = ["bronze", "silver", "gold"]

        # Fase 1: Limpeza em paralelo por camada
        cleanup_tasks = [
            cleanup_old_partitions(layer, retention_days) for layer in layers
        ]

        # Fase 2: Otimiza√ß√£o em paralelo por camada
        optimization_tasks = [optimize_delta_tables(layer) for layer in layers]

        # Fase 3: Auditoria de qualidade em paralelo por camada
        audit_tasks = [data_quality_audit(layer) for layer in layers]

        # Fase 4: Estat√≠sticas e backup (sequencial)
        usage_stats = generate_usage_statistics()
        backup_result = backup_critical_data()

        # Fase 5: Relat√≥rio consolidado
        maintenance_report = create_maintenance_report(
            cleanup_tasks, optimization_tasks, audit_tasks, usage_stats, backup_result
        )

        # Resultado final
        pipeline_result = {
            "status": "success",
            "maintenance_report": maintenance_report,
            "cleanup_results": cleanup_tasks,
            "optimization_results": optimization_tasks,
            "audit_results": audit_tasks,
            "usage_stats": usage_stats,
            "backup_result": backup_result,
            "completed_at": datetime.now().isoformat(),
        }

        run_logger.info("üéâ Manuten√ß√£o do Data Lake conclu√≠da com sucesso!")
        return pipeline_result

    except Exception as e:
        run_logger.error(f"‚ùå Falha na manuten√ß√£o: {str(e)}")
        raise


if __name__ == "__main__":
    # Criar deployment para execu√ß√£o semanal (s√°bados √†s 03:00)
    from prefect.deployments import Deployment
    from prefect.server.schemas.schedules import CronSchedule

    deployment = Deployment.build_from_flow(
        flow=data_lake_maintenance_flow,
        name="datalake-maintenance-weekly",
        version="1.0.0",
        schedule=CronSchedule(cron="0 3 * * 6", timezone="America/Sao_Paulo"),
        work_pool_name="default-agent-pool",
        tags=["maintenance", "cleanup", "weekly", "datalake"],
        parameters={"retention_days": 30},
    )

    deployment.apply()
    print("‚úÖ Deployment 'datalake-maintenance-weekly' criado com sucesso!")
