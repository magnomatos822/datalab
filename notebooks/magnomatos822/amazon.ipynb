{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85f3aff",
   "metadata": {},
   "source": [
    "# Análise de Dados Financeiros da Amazon\n",
    "\n",
    "Neste notebook, vamos analisar dados financeiros históricos da Amazon usando a arquitetura Medallion do DataFlow Lab. Os dados serão processados através das camadas Bronze, Silver e Gold, e então utilizados para análises e modelagem preditiva."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279922b",
   "metadata": {},
   "source": [
    "## Configuração do Ambiente\n",
    "\n",
    "Primeiro, vamos configurar o ambiente Spark com suporte ao Delta Lake e conexão ao MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e679dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, sum, count, window, lag, date_format\n",
    "from pyspark.sql.types import DoubleType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar SparkSession com Delta Lake e MinIO\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"AmazonFinancialAnalysis\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"admin123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905f545c",
   "metadata": {},
   "source": [
    "## Ingestão de Dados (Camada Bronze)\n",
    "\n",
    "Vamos ingerir dados históricos de ações da Amazon e salvá-los na camada Bronze sem transformações significativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98963e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixar dados históricos da Amazon (exemplo)\n",
    "# Em um caso real, esses dados seriam obtidos de uma API financeira como Yahoo Finance, Alpha Vantage, etc.\n",
    "# Para este exemplo, vamos simular criando um DataFrame\n",
    "\n",
    "# Criar dados simulados para demonstração\n",
    "data = [\n",
    "    (\"2025-01-02\", 3400.50, 3450.00, 3380.00, 3440.25, 2500000),\n",
    "    (\"2025-01-03\", 3440.25, 3500.00, 3430.00, 3490.75, 3000000),\n",
    "    (\"2025-01-04\", 3490.75, 3520.00, 3470.00, 3510.50, 2800000),\n",
    "    (\"2025-01-05\", 3510.50, 3550.00, 3490.00, 3540.00, 3200000),\n",
    "    (\"2025-01-06\", 3540.00, 3580.00, 3520.00, 3570.25, 2900000),\n",
    "    (\"2025-01-09\", 3570.25, 3600.00, 3550.00, 3590.50, 2600000),\n",
    "    (\"2025-01-10\", 3590.50, 3620.00, 3570.00, 3610.75, 2700000),\n",
    "    (\"2025-01-11\", 3610.75, 3650.00, 3600.00, 3640.00, 3100000),\n",
    "    (\"2025-01-12\", 3640.00, 3670.00, 3620.00, 3660.50, 2800000),\n",
    "    (\"2025-01-13\", 3660.50, 3700.00, 3640.00, 3685.75, 3300000),\n",
    "    (\"2025-01-16\", 3685.75, 3710.00, 3660.00, 3695.00, 2500000),\n",
    "    (\"2025-01-17\", 3695.00, 3730.00, 3680.00, 3720.25, 2900000),\n",
    "    (\"2025-01-18\", 3720.25, 3750.00, 3700.00, 3740.50, 3000000),\n",
    "    (\"2025-01-19\", 3740.50, 3770.00, 3720.00, 3760.75, 2800000),\n",
    "    (\"2025-01-20\", 3760.75, 3800.00, 3740.00, 3780.00, 3200000),\n",
    "    (\"2025-01-23\", 3780.00, 3810.00, 3760.00, 3790.25, 2700000),\n",
    "    (\"2025-01-24\", 3790.25, 3820.00, 3770.00, 3810.50, 2900000),\n",
    "    (\"2025-01-25\", 3810.50, 3840.00, 3790.00, 3830.75, 3100000),\n",
    "    (\"2025-01-26\", 3830.75, 3860.00, 3810.00, 3850.00, 3000000),\n",
    "    (\"2025-01-27\", 3850.00, 3880.00, 3830.00, 3870.25, 3200000),\n",
    "    (\"2025-01-30\", 3870.25, 3900.00, 3850.00, 3890.50, 2800000),\n",
    "    (\"2025-01-31\", 3890.50, 3920.00, 3870.00, 3910.75, 3000000),\n",
    "    (\"2025-02-01\", 3910.75, 3940.00, 3890.00, 3930.00, 3100000),\n",
    "    (\"2025-02-02\", 3930.00, 3950.00, 3900.00, 3940.25, 2700000),\n",
    "    (\"2025-02-03\", 3940.25, 3970.00, 3920.00, 3960.50, 2900000),\n",
    "    (\"2025-02-06\", 3960.50, 3990.00, 3940.00, 3980.75, 3000000),\n",
    "    (\"2025-02-07\", 3980.75, 4010.00, 3960.00, 4000.00, 3300000),\n",
    "    (\"2025-02-08\", 4000.00, 4030.00, 3980.00, 4020.25, 3500000),\n",
    "    (\"2025-02-09\", 4020.25, 4050.00, 4000.00, 4040.50, 3200000),\n",
    "    (\"2025-02-10\", 4040.50, 4070.00, 4020.00, 4060.75, 3100000),\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "schema = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "raw_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Converter coluna de data para tipo correto\n",
    "raw_df = raw_df.withColumn(\"date\", col(\"date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c30f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar metadados para a camada Bronze\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "bronze_df = (\n",
    "    raw_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source\", lit(\"yahoo_finance\"))\n",
    "    .withColumn(\"symbol\", lit(\"AMZN\"))\n",
    ")\n",
    "\n",
    "# Salvar na camada Bronze\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://bronze/stock_data/amazon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f2959",
   "metadata": {},
   "source": [
    "## Processamento para a Camada Silver\n",
    "\n",
    "Vamos processar os dados da camada Bronze para a camada Silver, realizando limpeza e transformações necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06396959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler dados da camada Bronze\n",
    "bronze_data = spark.read.format(\"delta\").load(\"s3a://bronze/stock_data/amazon\")\n",
    "\n",
    "# Processamento para a camada Silver\n",
    "# - Converter tipos de dados\n",
    "# - Remover dados inválidos\n",
    "# - Adicionar colunas derivadas\n",
    "\n",
    "silver_df = (\n",
    "    bronze_data.withColumn(\"open\", col(\"open\").cast(DoubleType()))\n",
    "    .withColumn(\"high\", col(\"high\").cast(DoubleType()))\n",
    "    .withColumn(\"low\", col(\"low\").cast(DoubleType()))\n",
    "    .withColumn(\"close\", col(\"close\").cast(DoubleType()))\n",
    "    .withColumn(\"volume\", col(\"volume\").cast(DoubleType()))\n",
    "    .withColumn(\"day_of_week\", date_format(col(\"date\"), \"E\"))\n",
    "    .withColumn(\"month\", date_format(col(\"date\"), \"MM\"))\n",
    ")\n",
    "\n",
    "# Calcular colunas derivadas\n",
    "silver_df = (\n",
    "    silver_df.withColumn(\n",
    "        \"daily_return\", (col(\"close\") - col(\"open\")) / col(\"open\") * 100\n",
    "    )\n",
    "    .withColumn(\"range\", col(\"high\") - col(\"low\"))\n",
    "    .withColumn(\"is_up_day\", col(\"close\") > col(\"open\"))\n",
    ")\n",
    "\n",
    "# Filtrar dados inválidos\n",
    "silver_df = (\n",
    "    silver_df.filter(col(\"open\") > 0)\n",
    "    .filter(col(\"high\") > col(\"low\"))\n",
    "    .filter(col(\"volume\") > 0)\n",
    ")\n",
    "\n",
    "# Salvar na camada Silver\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://silver/stock_data/amazon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09b793",
   "metadata": {},
   "source": [
    "## Análise e Agregação para a Camada Gold\n",
    "\n",
    "Vamos processar os dados da camada Silver para a camada Gold, realizando agregações e criando métricas analíticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler dados da camada Silver\n",
    "silver_data = spark.read.format(\"delta\").load(\"s3a://silver/stock_data/amazon\")\n",
    "\n",
    "# Calcular métricas diárias\n",
    "daily_metrics = silver_data.select(\n",
    "    \"date\",\n",
    "    \"symbol\",\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"daily_return\",\n",
    "    \"range\",\n",
    "    \"is_up_day\",\n",
    ")\n",
    "\n",
    "# Calcular métricas móveis (médias móveis de 5 dias)\n",
    "window_spec = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-4, 0)\n",
    "\n",
    "moving_metrics = daily_metrics.withColumn(\n",
    "    \"ma5_close\", avg(col(\"close\")).over(window_spec)\n",
    ").withColumn(\"ma5_volume\", avg(col(\"volume\")).over(window_spec))\n",
    "\n",
    "# Calcular variação diária\n",
    "window_lag = Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "gold_df = moving_metrics.withColumn(\n",
    "    \"prev_close\", lag(col(\"close\"), 1).over(window_lag)\n",
    ").withColumn(\n",
    "    \"daily_change_pct\", (col(\"close\") - col(\"prev_close\")) / col(\"prev_close\") * 100\n",
    ")\n",
    "\n",
    "# Filtrar valores nulos (primeiro dia não terá variação calculável)\n",
    "gold_df = gold_df.filter(col(\"daily_change_pct\").isNotNull())\n",
    "\n",
    "# Salvar na camada Gold\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"s3a://gold/stock_data/amazon_metrics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0751eab",
   "metadata": {},
   "source": [
    "## Análise Exploratória\n",
    "\n",
    "Vamos usar os dados da camada Gold para realizar algumas análises exploratórias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados da camada Gold\n",
    "gold_data = spark.read.format(\"delta\").load(\"s3a://gold/stock_data/amazon_metrics\")\n",
    "\n",
    "# Converter para Pandas para visualização\n",
    "pd_df = gold_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização do preço de fechamento e média móvel de 5 dias\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pd_df[\"date\"], pd_df[\"close\"], label=\"Preço de Fechamento\")\n",
    "plt.plot(\n",
    "    pd_df[\"date\"], pd_df[\"ma5_close\"], label=\"Média Móvel de 5 dias\", linestyle=\"--\"\n",
    ")\n",
    "plt.title(\"Preço da Ação da Amazon - Janeiro/Fevereiro 2025\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Preço ($)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise da variação percentual diária\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(\n",
    "    pd_df[\"date\"],\n",
    "    pd_df[\"daily_change_pct\"],\n",
    "    color=[\"g\" if x > 0 else \"r\" for x in pd_df[\"daily_change_pct\"]],\n",
    ")\n",
    "plt.title(\"Variação Percentual Diária da Ação da Amazon\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Variação (%)\")\n",
    "plt.axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume de negociação e média móvel de volume\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(pd_df[\"date\"], pd_df[\"volume\"], alpha=0.5, label=\"Volume\")\n",
    "plt.plot(\n",
    "    pd_df[\"date\"],\n",
    "    pd_df[\"ma5_volume\"],\n",
    "    color=\"orange\",\n",
    "    label=\"Média Móvel Volume (5 dias)\",\n",
    ")\n",
    "plt.title(\"Volume de Negociação da Amazon\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Volume\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd64a2",
   "metadata": {},
   "source": [
    "## Estatísticas e Insights\n",
    "\n",
    "Vamos extrair algumas estatísticas e insights dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61235879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas descritivas\n",
    "print(\"Estatísticas dos Preços de Fechamento:\")\n",
    "pd_df[\"close\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3099721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de dias positivos vs. negativos\n",
    "dias_positivos = pd_df[\"daily_change_pct\"].apply(lambda x: x > 0).sum()\n",
    "dias_negativos = pd_df[\"daily_change_pct\"].apply(lambda x: x <= 0).sum()\n",
    "total_dias = len(pd_df)\n",
    "\n",
    "print(\n",
    "    f\"Dias com variação positiva: {dias_positivos} ({dias_positivos/total_dias*100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Dias com variação negativa: {dias_negativos} ({dias_negativos/total_dias*100:.2f}%)\"\n",
    ")\n",
    "\n",
    "# Calcular retorno médio diário\n",
    "retorno_medio = pd_df[\"daily_change_pct\"].mean()\n",
    "print(f\"Retorno médio diário: {retorno_medio:.4f}%\")\n",
    "\n",
    "# Calcular volatilidade (desvio padrão do retorno diário)\n",
    "volatilidade = pd_df[\"daily_change_pct\"].std()\n",
    "print(f\"Volatilidade diária: {volatilidade:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c94d52",
   "metadata": {},
   "source": [
    "## Modelagem Preditiva\n",
    "\n",
    "Vamos criar um modelo simples para prever o preço de fechamento usando regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "mlflow.set_experiment(\"amazon_stock_prediction\")\n",
    "\n",
    "# Preparar features\n",
    "feature_cols = [\"open\", \"high\", \"low\", \"volume\", \"ma5_close\", \"ma5_volume\"]\n",
    "label_col = \"close\"\n",
    "\n",
    "# Criar assembler para combinar features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em conjuntos de treino e teste (70% treino, 30% teste)\n",
    "train_data, test_data = assembled_data.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Treinar modelo de regressão linear\n",
    "with mlflow.start_run(run_name=\"linear_regression_model\"):\n",
    "    # Criar e treinar o modelo\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=label_col)\n",
    "    lr_model = lr.fit(train_data)\n",
    "\n",
    "    # Fazer previsões\n",
    "    predictions = lr_model.transform(test_data)\n",
    "\n",
    "    # Avaliar o modelo\n",
    "    evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\")\n",
    "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "    r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "\n",
    "    # Registrar parâmetros e métricas no MLflow\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"features\": \", \".join(feature_cols),\n",
    "            \"elasticNetParam\": lr.getElasticNetParam(),\n",
    "            \"regParam\": lr.getRegParam(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # Salvar modelo\n",
    "    mlflow.spark.log_model(lr_model, \"model\")\n",
    "\n",
    "    # Registrar coeficientes\n",
    "    coefficients = {\n",
    "        feature: coef for feature, coef in zip(feature_cols, lr_model.coefficients)\n",
    "    }\n",
    "    mlflow.log_params({f\"coef_{k}\": v for k, v in coefficients.items()})\n",
    "    mlflow.log_param(\"intercept\", lr_model.intercept)\n",
    "\n",
    "    # Imprimir resultados\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R²: {r2}\")\n",
    "    print(f\"Coeficientes: {coefficients}\")\n",
    "    print(f\"Intercepto: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar previsões vs. valores reais\n",
    "pred_df = predictions.select(\"date\", label_col, \"prediction\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(pred_df[\"date\"], pred_df[label_col], \"o-\", label=\"Valor Real\")\n",
    "plt.plot(pred_df[\"date\"], pred_df[\"prediction\"], \"o--\", label=\"Previsão\")\n",
    "plt.title(\"Previsão do Preço de Fechamento da Ação da Amazon\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Preço ($)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6820992",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste notebook, demonstramos o fluxo completo de dados através da arquitetura Medallion no DataFlow Lab:\n",
    "\n",
    "1. **Camada Bronze**: Ingestão de dados brutos da ação da Amazon\n",
    "2. **Camada Silver**: Transformação, limpeza e enriquecimento dos dados\n",
    "3. **Camada Gold**: Agregação e criação de métricas analíticas\n",
    "\n",
    "Além disso, realizamos:\n",
    "- Análise exploratória dos dados com visualizações\n",
    "- Extração de insights estatísticos\n",
    "- Modelagem preditiva de preços com MLflow para rastreamento\n",
    "\n",
    "Este fluxo demonstra a eficácia da arquitetura Medallion do DataFlow Lab para análises financeiras, combinando a confiabilidade do Delta Lake com o poder analítico do Spark e a rastreabilidade do MLflow."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
