{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a579b3f5",
   "metadata": {},
   "source": [
    "# Análise de Vendas Retail com Arquitetura Medallion\n",
    "\n",
    "Este notebook demonstra como utilizar a arquitetura Medallion (Bronze, Silver, Gold) do DataFlow Lab para uma análise de dados de varejo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f676c69",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "Primeiro, vamos configurar nossa sessão Spark com suporte ao Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    year,\n",
    "    month,\n",
    "    dayofmonth,\n",
    "    hour,\n",
    "    date_format,\n",
    "    to_date,\n",
    "    lit,\n",
    "    count,\n",
    "    sum,\n",
    "    avg,\n",
    "    when,\n",
    "    expr,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configuração com suporte ao Delta Lake\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Retail-Sales-Analysis\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Sessão Spark iniciada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45920a0",
   "metadata": {},
   "source": [
    "## 2. Geração de Dados de Exemplo\n",
    "\n",
    "Vamos criar um conjunto de dados simulado de vendas de varejo para nossa análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1cee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando dados de exemplo\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Lista de produtos\n",
    "products = [\n",
    "    {\"id\": 1, \"name\": \"Smartphone\", \"category\": \"Eletrônicos\", \"price\": 1999.90},\n",
    "    {\"id\": 2, \"name\": \"Notebook\", \"category\": \"Eletrônicos\", \"price\": 3499.90},\n",
    "    {\"id\": 3, \"name\": \"Tênis\", \"category\": \"Vestuário\", \"price\": 299.90},\n",
    "    {\"id\": 4, \"name\": \"Camiseta\", \"category\": \"Vestuário\", \"price\": 99.90},\n",
    "    {\"id\": 5, \"name\": \"Panela Elétrica\", \"category\": \"Casa\", \"price\": 399.90},\n",
    "    {\"id\": 6, \"name\": \"Liquidificador\", \"category\": \"Casa\", \"price\": 149.90},\n",
    "    {\"id\": 7, \"name\": \"Livro\", \"category\": \"Cultura\", \"price\": 49.90},\n",
    "    {\"id\": 8, \"name\": \"Fones de Ouvido\", \"category\": \"Eletrônicos\", \"price\": 199.90},\n",
    "]\n",
    "\n",
    "# Lista de lojas\n",
    "stores = [\n",
    "    {\"id\": 1, \"name\": \"Loja Centro\", \"city\": \"São Paulo\", \"region\": \"Sudeste\"},\n",
    "    {\"id\": 2, \"name\": \"Loja Shopping\", \"city\": \"Rio de Janeiro\", \"region\": \"Sudeste\"},\n",
    "    {\"id\": 3, \"name\": \"Loja Norte\", \"city\": \"Salvador\", \"region\": \"Nordeste\"},\n",
    "    {\"id\": 4, \"name\": \"Loja Sul\", \"city\": \"Porto Alegre\", \"region\": \"Sul\"},\n",
    "]\n",
    "\n",
    "# Gerar dados de vendas\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 12, 31)\n",
    "sales_data = []\n",
    "\n",
    "# Adicionar ID para facilitar a simulação de problemas de qualidade de dados\n",
    "sale_id = 1\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Número de vendas por dia varia entre 50 e 200\n",
    "    daily_sales = random.randint(50, 200)\n",
    "\n",
    "    for _ in range(daily_sales):\n",
    "        product = random.choice(products)\n",
    "        store = random.choice(stores)\n",
    "        quantity = random.randint(1, 5)\n",
    "\n",
    "        # Adicionar ruído nos dados (valores nulos, formatação incorreta etc.)\n",
    "        if random.random() < 0.05:  # 5% de chance de ter um problema de qualidade\n",
    "            problem_type = random.choice(\n",
    "                [\"null_price\", \"negative_quantity\", \"wrong_date\", \"null_store\"]\n",
    "            )\n",
    "\n",
    "            if problem_type == \"null_price\":\n",
    "                price = None\n",
    "            elif problem_type == \"negative_quantity\":\n",
    "                quantity = -quantity\n",
    "            elif problem_type == \"wrong_date\":\n",
    "                sale_date = \"INVALID_DATE\"\n",
    "            elif problem_type == \"null_store\":\n",
    "                store_id = None\n",
    "                store_name = None\n",
    "                store_city = None\n",
    "                store_region = None\n",
    "            else:\n",
    "                price = product[\"price\"]\n",
    "                sale_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "                store_id = store[\"id\"]\n",
    "                store_name = store[\"name\"]\n",
    "                store_city = store[\"city\"]\n",
    "                store_region = store[\"region\"]\n",
    "        else:\n",
    "            price = product[\"price\"]\n",
    "            sale_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "            store_id = store[\"id\"]\n",
    "            store_name = store[\"name\"]\n",
    "            store_city = store[\"city\"]\n",
    "            store_region = store[\"region\"]\n",
    "\n",
    "        # Criar registro de venda\n",
    "        sale = {\n",
    "            \"sale_id\": sale_id,\n",
    "            \"sale_date\": sale_date,\n",
    "            \"product_id\": product[\"id\"],\n",
    "            \"product_name\": product[\"name\"],\n",
    "            \"product_category\": product[\"category\"],\n",
    "            \"price\": price,\n",
    "            \"quantity\": quantity,\n",
    "            \"store_id\": store_id,\n",
    "            \"store_name\": store_name,\n",
    "            \"store_city\": store_city,\n",
    "            \"store_region\": store_region,\n",
    "            \"total_amount\": None if price is None else price * quantity,\n",
    "        }\n",
    "\n",
    "        sales_data.append(sale)\n",
    "        sale_id += 1\n",
    "\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Converter para DataFrame do Pandas\n",
    "sales_df_pd = pd.DataFrame(sales_data)\n",
    "\n",
    "# Converter para DataFrame do Spark\n",
    "sales_df = spark.createDataFrame(sales_df_pd)\n",
    "\n",
    "print(f\"Dados gerados: {sales_df.count()} registros de vendas\")\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9d93d",
   "metadata": {},
   "source": [
    "## 3. Camada Bronze - Armazenamento de Dados Brutos\n",
    "\n",
    "Na camada Bronze, salvamos os dados brutos sem transformações além das necessárias para conformidade de formato. Esta camada serve como o principal ponto de entrada de dados no lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab03a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o caminho para armazenamento\n",
    "bronze_path = \"s3a://bronze/retail/sales\"\n",
    "\n",
    "# Escrever na camada Bronze no formato Delta\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
    "\n",
    "print(f\"Dados salvos na camada Bronze: {bronze_path}\")\n",
    "\n",
    "# Verificar os dados armazenados\n",
    "bronze_df = spark.read.format(\"delta\").load(bronze_path)\n",
    "print(f\"Registros na camada Bronze: {bronze_df.count()}\")\n",
    "bronze_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4ec28",
   "metadata": {},
   "source": [
    "## 4. Camada Silver - Dados Limpos e Validados\n",
    "\n",
    "Na camada Silver, aplicamos transformações para limpar e validar os dados. Esta camada fornece dados confiáveis de alta qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ad86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processamento para camada Silver\n",
    "silver_df = (\n",
    "    bronze_df.withColumn(\"sale_date\", to_date(col(\"sale_date\")))\n",
    "    .filter(col(\"quantity\") > 0)\n",
    "    .filter(col(\"price\").isNotNull())\n",
    "    .filter(col(\"store_id\").isNotNull())\n",
    "    .withColumn(\"year\", year(col(\"sale_date\")))\n",
    "    .withColumn(\"month\", month(col(\"sale_date\")))\n",
    "    .withColumn(\"day\", dayofmonth(col(\"sale_date\")))\n",
    "    .withColumn(\"total_amount\", col(\"price\") * col(\"quantity\"))\n",
    ")\n",
    "\n",
    "# Definir o caminho para armazenamento Silver\n",
    "silver_path = \"s3a://silver/retail/sales\"\n",
    "\n",
    "# Escrever na camada Silver\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
    "\n",
    "print(f\"Dados salvos na camada Silver: {silver_path}\")\n",
    "\n",
    "# Verificar os dados Silver\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "print(f\"Registros na camada Silver: {silver_df.count()}\")\n",
    "silver_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d57dac",
   "metadata": {},
   "source": [
    "## 5. Camada Gold - Dados Agregados e Prontos para Negócios\n",
    "\n",
    "Na camada Gold, criamos visões agregadas e preparadas para análise de negócios e ciência de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar visões agregadas para a camada Gold\n",
    "\n",
    "# Vendas por categoria e região\n",
    "sales_by_category_region = silver_df.groupBy(\n",
    "    \"product_category\", \"store_region\", \"year\", \"month\"\n",
    ").agg(\n",
    "    count(\"sale_id\").alias(\"total_sales\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    ")\n",
    "\n",
    "# Vendas por loja e dia\n",
    "sales_by_store_daily = silver_df.groupBy(\"store_id\", \"store_name\", \"sale_date\").agg(\n",
    "    count(\"sale_id\").alias(\"daily_sales\"), sum(\"total_amount\").alias(\"daily_revenue\")\n",
    ")\n",
    "\n",
    "# Top produtos por vendas\n",
    "top_products = (\n",
    "    silver_df.groupBy(\"product_id\", \"product_name\", \"product_category\")\n",
    "    .agg(\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    )\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    ")\n",
    "\n",
    "# Definir caminhos para armazenamento Gold\n",
    "gold_path_category_region = \"s3a://gold/retail/sales_by_category_region\"\n",
    "gold_path_store_daily = \"s3a://gold/retail/sales_by_store_daily\"\n",
    "gold_path_top_products = \"s3a://gold/retail/top_products\"\n",
    "\n",
    "# Escrever na camada Gold\n",
    "sales_by_category_region.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    gold_path_category_region\n",
    ")\n",
    "sales_by_store_daily.write.format(\"delta\").mode(\"overwrite\").save(gold_path_store_daily)\n",
    "top_products.write.format(\"delta\").mode(\"overwrite\").save(gold_path_top_products)\n",
    "\n",
    "print(\"Dados agregados salvos na camada Gold\")\n",
    "\n",
    "# Visualizar alguns dados Gold\n",
    "print(\"\\nVendas por Categoria e Região:\")\n",
    "spark.read.format(\"delta\").load(gold_path_category_region).show(5)\n",
    "\n",
    "print(\"\\nTop Produtos:\")\n",
    "spark.read.format(\"delta\").load(gold_path_top_products).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d77c8a",
   "metadata": {},
   "source": [
    "## 6. Análise e Visualização dos Dados\n",
    "\n",
    "Vamos criar algumas visualizações utilizando os dados da camada Gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59891d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para Pandas para visualização\n",
    "category_region_pd = (\n",
    "    spark.read.format(\"delta\").load(gold_path_category_region).toPandas()\n",
    ")\n",
    "top_products_pd = (\n",
    "    spark.read.format(\"delta\").load(gold_path_top_products).toPandas().head(10)\n",
    ")\n",
    "store_daily_pd = spark.read.format(\"delta\").load(gold_path_store_daily).toPandas()\n",
    "\n",
    "# Configuração de visualização\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "\n",
    "# 1. Vendas por Categoria\n",
    "category_sales = (\n",
    "    category_region_pd.groupby(\"product_category\")[\"total_revenue\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=category_sales.index, y=category_sales.values)\n",
    "plt.title(\"Receita Total por Categoria de Produto\", fontsize=16)\n",
    "plt.xlabel(\"Categoria\")\n",
    "plt.ylabel(\"Receita Total (R$)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Vendas por Região\n",
    "region_sales = (\n",
    "    category_region_pd.groupby(\"store_region\")[\"total_revenue\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=region_sales.index, y=region_sales.values, palette=\"viridis\")\n",
    "plt.title(\"Receita Total por Região\", fontsize=16)\n",
    "plt.xlabel(\"Região\")\n",
    "plt.ylabel(\"Receita Total (R$)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Top 10 Produtos\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(\n",
    "    x=\"product_name\", y=\"total_revenue\", data=top_products_pd, palette=\"coolwarm\"\n",
    ")\n",
    "plt.title(\"Top 10 Produtos por Receita Total\", fontsize=16)\n",
    "plt.xlabel(\"Produto\")\n",
    "plt.ylabel(\"Receita Total (R$)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Tendência de vendas diárias\n",
    "# Convertendo para o formato de data correto\n",
    "store_daily_pd[\"sale_date\"] = pd.to_datetime(store_daily_pd[\"sale_date\"])\n",
    "\n",
    "# Agrupando por data para obter o total de vendas diárias em todas as lojas\n",
    "daily_sales = store_daily_pd.groupby(\"sale_date\")[\"daily_revenue\"].sum().reset_index()\n",
    "daily_sales = daily_sales.sort_values(\"sale_date\")\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(\n",
    "    daily_sales[\"sale_date\"],\n",
    "    daily_sales[\"daily_revenue\"],\n",
    "    marker=\"o\",\n",
    "    markersize=3,\n",
    "    linestyle=\"-\",\n",
    ")\n",
    "plt.title(\"Tendência de Vendas Diárias\", fontsize=16)\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Receita Diária (R$)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a50539",
   "metadata": {},
   "source": [
    "## 7. Aplicação de Machine Learning - Previsão de Vendas\n",
    "\n",
    "Vamos utilizar os dados da camada Silver para treinar um modelo simples de previsão de vendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e37f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "mlflow.set_experiment(\"Previsão de Vendas Varejo\")\n",
    "\n",
    "# Preparar dados para ML a partir da camada Silver\n",
    "# Vamos prever a quantidade vendida com base nas características do produto e da loja\n",
    "\n",
    "# Usar dados da camada Silver\n",
    "ml_data = silver_df.select(\n",
    "    \"product_id\",\n",
    "    \"product_category\",\n",
    "    \"store_id\",\n",
    "    \"store_region\",\n",
    "    \"price\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"quantity\",\n",
    ").toPandas()\n",
    "\n",
    "# Separar features e target\n",
    "X = ml_data.drop(\"quantity\", axis=1)\n",
    "y = ml_data[\"quantity\"]\n",
    "\n",
    "# Preprocessamento - Codificar variáveis categóricas\n",
    "categorical_cols = [\"product_category\", \"store_region\"]\n",
    "numerical_cols = [\"product_id\", \"store_id\", \"price\", \"month\", \"day\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numerical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Iniciar experimento MLflow\n",
    "with mlflow.start_run(run_name=\"RF_Vendas_Varejo\"):\n",
    "\n",
    "    # Definir hiperparâmetros\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    min_samples_split = 5\n",
    "\n",
    "    # Logar parâmetros\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "\n",
    "    # Criar pipeline com preprocessamento e modelo\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\n",
    "                \"regressor\",\n",
    "                RandomForestRegressor(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Treinar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Fazer predições\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular métricas\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Métricas do Modelo:\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    # Logar métricas\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Registrar o modelo\n",
    "    mlflow.sklearn.log_model(model, \"retail_sales_model\")\n",
    "\n",
    "    print(f\"Modelo registrado no MLflow com sucesso!\")\n",
    "    print(\n",
    "        f\"Acesse a interface do MLflow em http://localhost:5000 para ver os detalhes do experimento.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d8bf9",
   "metadata": {},
   "source": [
    "## 8. Conclusão\n",
    "\n",
    "Neste notebook, demonstramos o fluxo completo de dados através da arquitetura Medallion do DataFlow Lab:\n",
    "\n",
    "1. **Bronze**: Armazenamos os dados brutos sem transformações significativas\n",
    "2. **Silver**: Aplicamos limpeza e validações para garantir qualidade\n",
    "3. **Gold**: Criamos visões agregadas prontas para análise de negócios\n",
    "\n",
    "Além disso, utilizamos os dados para:\n",
    "- Criar visualizações de negócios relevantes\n",
    "- Treinar um modelo de machine learning para previsão de vendas\n",
    "- Rastrear experimentos e versões de modelos com MLflow\n",
    "\n",
    "Esta arquitetura oferece várias vantagens:\n",
    "- **Rastreabilidade**: Podemos rastrear os dados desde a origem até seu uso final\n",
    "- **Qualidade**: Validação progressiva em cada camada\n",
    "- **Performance**: Dados pré-agregados para consultas rápidas\n",
    "- **Governança**: Separação clara entre dados brutos e processados\n",
    "\n",
    "Para próximos passos, considere:\n",
    "- Implementar um pipeline de atualização incremental\n",
    "- Adicionar validações de qualidade mais rigorosas\n",
    "- Explorar modelos de machine learning mais avançados\n",
    "- Criar dashboards no Streamlit conectados à camada Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerrar a sessão Spark\n",
    "spark.stop()\n",
    "print(\"Sessão Spark encerrada.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
