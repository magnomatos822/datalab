version: '3.8'

# DataLab - Complete Data Science Platform
# Main docker-compose file that orchestrates all services

services:
  # ====================================
  # CORE SERVICES (Storage & Messaging)
  # ====================================
  
  # MinIO (S3-compatible object storage)
  minio:
    image: bitnami/minio:latest
    container_name: minio
    environment:
      - MINIO_ROOT_USER=${AWS_ACCESS_KEY_ID:-minioadmin}
      - MINIO_ROOT_PASSWORD=${AWS_SECRET_ACCESS_KEY:-minioadmin}
      - MINIO_DAEMON_USER=root
      - MINIO_DAEMON_GROUP=root
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/bitnami/minio/data
      - ./config/minio:/docker-entrypoint-initdb.d
    networks:
      - dataflow-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Kafka (Event Streaming)
  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_KRAFT_CLUSTER_ID=5L6g3nShT-eMCtK--X86sw
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
    ports:
      - "9092:9092"
      - "29092:29092"
    volumes:
      - kafka-data:/bitnami/kafka
    networks:
      - dataflow-network
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # ====================================
  # PROCESSING SERVICES (Spark)
  # ====================================
  
  # Spark Master
  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-minioadmin}
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./app:/opt/spark-apps
      - ./config/spark/jars:/opt/bitnami/spark/jars/custom
    networks:
      - dataflow-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
    depends_on:
      - minio
      - kafka

  # Spark Workers
  spark-worker-1:
    image: bitnami/spark:latest
    container_name: datalab-spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-minioadmin}
    volumes:
      - ./app:/opt/spark-apps
    networks:
      - dataflow-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    depends_on:
      - spark-master
      
  spark-worker-2:
    image: bitnami/spark:latest
    container_name: datalab-spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-minioadmin}
    volumes:
      - ./app:/opt/spark-apps
    networks:
      - dataflow-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    depends_on:
      - spark-master

  # ====================================
  # ML & WORKFLOW SERVICES
  # ====================================
  
  # MLflow (ML Lifecycle Management)
  mlflow:
    build:
      context: ./config/mlflow
      dockerfile: Dockerfile
    container_name: mlflow
    user: root
    command: ["/opt/bitnami/python/bin/mlflow", "server", "--host", "0.0.0.0", "--backend-store-uri", "sqlite:////opt/mlflow/mlflow.db", "--default-artifact-root", "s3://mlflow/artifacts/"]
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-minioadmin}
    volumes:
      - mlflow-data:/opt/mlflow
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - dataflow-network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    depends_on:
      - minio

  # Prefect (Workflow Orchestration)
  prefect:
    image: prefecthq/prefect:2.14.10-python3.11
    container_name: prefect
    command: ["prefect", "server", "start", "--host", "0.0.0.0"]
    ports:
      - "4200:4200"
    environment:
      - PREFECT_SERVER_API_HOST=0.0.0.0
      - PREFECT_API_URL=http://localhost:4200/api
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-minioadmin}
    volumes:
      - prefect-data:/opt/prefect
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4200/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - dataflow-network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    depends_on:
      - minio

  # ====================================
  # VISUALIZATION & ANALYSIS
  # ====================================
  
  # JupyterHub (Notebooks)
  jupyter:
    build:
      context: ./config/jupyterhub
      dockerfile: Dockerfile
    container_name: jupyterhub
    ports:
      - "8888:8000"
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-minioadmin}
      - S3_ENDPOINT=http://minio:9000
      - JUPYTER_S3_BUCKET=jupyter
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - dataflow-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - minio
      - mlflow

  # Streamlit (Web Apps)
  streamlit:
    build:
      context: ./config/streamlit
      dockerfile: Dockerfile
    container_name: streamlit
    ports:
      - "8501:8501"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-minioadmin}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-minioadmin}
      - S3_ENDPOINT=http://minio:9000
    volumes:
      - ./app:/app
    networks:
      - dataflow-network
    depends_on:
      - mlflow
      - minio

  # ====================================
  # MONITORING SERVICES
  # ====================================
  
  # Prometheus (Metrics Collection)
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - dataflow-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Grafana (Dashboards & Visualization)
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
    networks:
      - dataflow-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    depends_on:
      - prometheus

# ====================================
# NETWORKS
# ====================================
networks:
  dataflow-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# ====================================
# VOLUMES
# ====================================
volumes:
  minio-data:
    driver: local
  kafka-data:
    driver: local
  mlflow-data:
    driver: local
  prefect-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local